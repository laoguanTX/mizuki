---
title: "æ•°æ®ç§‘å­¦ä¸Žå·¥ç¨‹ä¼˜åŒ–(Code II)"
published: 2025-07-23
category: ["ç¨‹è®¾è®¡ç§‘", "äººå·¥æ™ºèƒ½"]
tags: ["äººå·¥æ™ºèƒ½"]
alias: "datascienceandengineeringoptimization(code1)"
---

## ä¸€ã€æ¦‚è¿°

è¿™ä¸ªPythonç¨‹åºå®žçŽ°äº†ç»å…¸çš„æ¢¯åº¦ä¸‹é™ç®—æ³•å’Œéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œå¹¶åœ¨ä¸¤ä¸ªä¸åŒçš„ä¼˜åŒ–é—®é¢˜ä¸Šè¿›è¡Œäº†æ¯”è¾ƒå®žéªŒï¼šRosenbrockå‡½æ•°å’Œå¼ºå‡¸äºŒæ¬¡å‡½æ•°ã€‚

## äºŒã€åŠŸèƒ½æ¨¡å—

### 1ã€æµ‹è¯•å‡½æ•°å®šä¹‰

#### Rosenbrockå‡½æ•°
- **å‡½æ•°**: `rosenbrock(w)`
- **æè¿°**: ç»å…¸çš„éžå‡¸ä¼˜åŒ–æµ‹è¯•å‡½æ•°ï¼Œä¹Ÿç§°ä¸º"é¦™è•‰å‡½æ•°"
- **æ•°å­¦è¡¨è¾¾å¼**: f(x,y) = (1-x)Â² + 100(y-xÂ²)Â²
- **æœ€ä¼˜è§£**: (1, 1)
- **ç‰¹ç‚¹**: å…·æœ‰ç‹­é•¿çš„å¼¯æ›²è°·åœ°ï¼Œæ˜¯æµ‹è¯•ä¼˜åŒ–ç®—æ³•æ€§èƒ½çš„ç»å…¸å‡½æ•°

#### å¼ºå‡¸äºŒæ¬¡å‡½æ•°
- **å‡½æ•°**: `quadratic(w, a=10)`
- **æè¿°**: å¼ºå‡¸äºŒæ¬¡å‡½æ•°ï¼Œå…·æœ‰è‰¯å¥½çš„ä¼˜åŒ–æ€§è´¨
- **æ•°å­¦è¡¨è¾¾å¼**: f(x,y) = 0.5(axÂ² + yÂ²)
- **æœ€ä¼˜è§£**: (0, 0)
- **å‚æ•°**: a > 0ï¼ŒæŽ§åˆ¶xæ–¹å‘çš„æ›²çŽ‡

### 2ã€æ¢¯åº¦è®¡ç®—

- `grad_rosenbrock(w)`: Rosenbrockå‡½æ•°çš„æ¢¯åº¦
- `grad_quadratic(w, a=10)`: äºŒæ¬¡å‡½æ•°çš„æ¢¯åº¦

### 3ã€ä¼˜åŒ–ç®—æ³•

#### æ ‡å‡†æ¢¯åº¦ä¸‹é™ (Gradient Descent)
**å‡½æ•°**: `gradient_descent(f, grad_f, w0, max_iter=1000, tol=1e-6, alpha=1e-3, use_line_search=False, **kwargs)`

**å‚æ•°è¯´æ˜Ž**:
- `f`: ç›®æ ‡å‡½æ•°
- `grad_f`: æ¢¯åº¦å‡½æ•°
- `w0`: åˆå§‹ç‚¹
- `max_iter`: æœ€å¤§è¿­ä»£æ¬¡æ•°
- `tol`: æ”¶æ•›å®¹å·®
- `alpha`: å­¦ä¹ çŽ‡ï¼ˆå›ºå®šæ­¥é•¿ï¼‰
- `use_line_search`: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”æ­¥é•¿

**ç‰¹ç‚¹**:
- ç¡®å®šæ€§ç®—æ³•ï¼Œæ¯æ¬¡ä½¿ç”¨ç²¾ç¡®æ¢¯åº¦
- æ”¯æŒå›ºå®šæ­¥é•¿å’Œè‡ªé€‚åº”æ­¥é•¿ä¸¤ç§æ¨¡å¼

#### éšæœºæ¢¯åº¦ä¸‹é™ (Stochastic Gradient Descent)
**å‡½æ•°**: `stochastic_gradient_descent(f, grad_f, w0, max_iter=1000, step=1e-3, add_noise=0.0, mult_noise=0.0, a=10)`

**å‚æ•°è¯´æ˜Ž**:
- `add_noise`: åŠ æ€§å™ªå£°å¼ºåº¦
- `mult_noise`: ä¹˜æ€§å™ªå£°å¼ºåº¦
- å…¶ä»–å‚æ•°ä¸Žæ¢¯åº¦ä¸‹é™ç›¸åŒ

**å™ªå£°æ¨¡åž‹**:
- åŠ æ€§å™ªå£°: g += add_noise * ð’©(0,I)
- ä¹˜æ€§å™ªå£°: g *= (1 + mult_noise * ð’©(0,I))

#### è‡ªé€‚åº”æ­¥é•¿æœç´¢
**å‡½æ•°**: `simple_line_search(f, grad_f, w, d, alpha0=1.0, c=1e-4, tau=0.5)`

å®žçŽ°äº†åŸºäºŽArmijoå‡†åˆ™çš„å›žæº¯çº¿æœç´¢ï¼š
- `alpha0`: åˆå§‹æ­¥é•¿
- `c`: Armijoå‚æ•°
- `tau`: æ­¥é•¿æ”¶ç¼©å› å­

### 4ã€å¯è§†åŒ–åŠŸèƒ½

#### ç­‰é«˜çº¿å›¾ä¸Žä¼˜åŒ–è·¯å¾„
**å‡½æ•°**: `plot_level_sets_and_path(f, hist, title, xmin=-2, xmax=2, ymin=-1, ymax=3, levels=30)`

- ç»˜åˆ¶å‡½æ•°çš„ç­‰é«˜çº¿å›¾
- æ˜¾ç¤ºä¼˜åŒ–ç®—æ³•çš„è¿­ä»£è·¯å¾„
- æ ‡è®°èµ·å§‹ç‚¹å’Œç»ˆç‚¹

#### æŸå¤±æ›²çº¿å¯¹æ¯”
**å‡½æ•°**: `plot_loss_curve(f, *paths, labels)`

- ç»˜åˆ¶ä¸åŒç®—æ³•çš„æ”¶æ•›æ›²çº¿
- ä½¿ç”¨å¯¹æ•°åæ ‡æ˜¾ç¤ºæŸå¤±å€¼å˜åŒ–

## äºŒã€å®žéªŒè®¾ç½®

### 1ã€å‚æ•°é…ç½®
```python
w0 = np.array([-1.5, 2.0])    # åˆå§‹ç‚¹
max_iter = 5000               # æœ€å¤§è¿­ä»£æ¬¡æ•°
tol = 1e-8                    # æ”¶æ•›å®¹å·®
alpha = 1e-3                  # å›ºå®šæ­¥é•¿
a_quad = 10                   # äºŒæ¬¡å‡½æ•°å‚æ•°
```

### 2ã€å®žéªŒå†…å®¹

1. **Rosenbrockå‡½æ•°ä¸Šçš„æ¢¯åº¦ä¸‹é™**
   - å›ºå®šæ­¥é•¿æ¢¯åº¦ä¸‹é™
   - å¸¦çº¿æœç´¢çš„æ¢¯åº¦ä¸‹é™

2. **å¼ºå‡¸äºŒæ¬¡å‡½æ•°ä¸Šçš„æ¢¯åº¦ä¸‹é™**
   - å›ºå®šæ­¥é•¿æ¢¯åº¦ä¸‹é™

3. **éšæœºæ¢¯åº¦ä¸‹é™å®žéªŒ**
   - Rosenbrockå‡½æ•° + åŠ æ€§å™ªå£°
   - äºŒæ¬¡å‡½æ•° + åŠ æ€§å™ªå£°
   - äºŒæ¬¡å‡½æ•° + ä¹˜æ€§å™ªå£°

4. **ç®—æ³•æ€§èƒ½å¯¹æ¯”**
   - æ”¶æ•›æ›²çº¿å¯¹æ¯”
   - è¿­ä»£æ­¥æ•°ç»Ÿè®¡

## ä¸‰ã€ä½¿ç”¨æ–¹æ³•

1. **ç›´æŽ¥è¿è¡Œ**:
   ```bash
   python SGD.py
   ```

2. **è‡ªå®šä¹‰å®žéªŒ**:
   ```python
   # åˆ›å»ºè‡ªå®šä¹‰ä¼˜åŒ–å®žéªŒ
   hist = gradient_descent(your_function, your_gradient, initial_point)
   plot_level_sets_and_path(your_function, hist, "Your Experiment")
   ```

## å››ã€è¾“å‡ºç»“æžœ

ç¨‹åºä¼šç”Ÿæˆä»¥ä¸‹å¯è§†åŒ–ç»“æžœï¼š
- Rosenbrockå‡½æ•°ä¸Šçš„ä¼˜åŒ–è·¯å¾„å›¾ï¼ˆå›ºå®šæ­¥é•¿ vs è‡ªé€‚åº”æ­¥é•¿ï¼‰
- äºŒæ¬¡å‡½æ•°ä¸Šçš„ä¼˜åŒ–è·¯å¾„å›¾
- SGDåœ¨ä¸åŒå™ªå£°è®¾ç½®ä¸‹çš„ä¼˜åŒ–è·¯å¾„å›¾
- ä¸åŒç®—æ³•çš„æŸå¤±æ”¶æ•›æ›²çº¿å¯¹æ¯”å›¾

## äº”ã€ä¾èµ–åº“

- `numpy`: æ•°å€¼è®¡ç®—
- `matplotlib`: ç»˜å›¾å¯è§†åŒ–

## å…­ã€ç®—æ³•ç‰¹ç‚¹åˆ†æž

### 1ã€æ¢¯åº¦ä¸‹é™ (GD)
- **ä¼˜ç‚¹**: æ”¶æ•›ç¨³å®šï¼Œç†è®ºä¿è¯å¼º
- **ç¼ºç‚¹**: å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢

### 2ã€éšæœºæ¢¯åº¦ä¸‹é™ (SGD)
- **ä¼˜ç‚¹**: å™ªå£°æœ‰åŠ©äºŽè·³å‡ºå±€éƒ¨æœ€ä¼˜ï¼Œé€‚åˆå¤§è§„æ¨¡é—®é¢˜
- **ç¼ºç‚¹**: æ”¶æ•›è·¯å¾„ä¸ç¨³å®šï¼Œå¯èƒ½åœ¨æœ€ä¼˜è§£é™„è¿‘éœ‡è¡

### 3ã€è‡ªé€‚åº”æ­¥é•¿
- **ä¼˜ç‚¹**: è‡ªåŠ¨è°ƒèŠ‚æ­¥é•¿ï¼Œé¿å…æ­¥é•¿é€‰æ‹©é—®é¢˜
- **ç¼ºç‚¹**: æ¯æ¬¡è¿­ä»£è®¡ç®—å¼€é”€æ›´å¤§

## ä¸ƒã€æ³¨æ„äº‹é¡¹

1. ä¸åŒå‡½æ•°éœ€è¦ä¸åŒçš„å­¦ä¹ çŽ‡è®¾ç½®
2. å™ªå£°å¼ºåº¦éœ€è¦æ ¹æ®å…·ä½“é—®é¢˜è°ƒèŠ‚
3. Rosenbrockå‡½æ•°æ¯”äºŒæ¬¡å‡½æ•°æ›´éš¾ä¼˜åŒ–
4. å®žé™…åº”ç”¨ä¸­å»ºè®®ç»“åˆå¤šç§ç­–ç•¥ï¼ˆåŠ¨é‡ã€è‡ªé€‚åº”å­¦ä¹ çŽ‡ç­‰ï¼‰

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. æµ‹è¯•å‡½æ•°å’Œæ¢¯åº¦çš„å®šä¹‰
def rosenbrock(w):
    """ Rosenbrock function """
    x, y = w
    return (1 - x)**2 + 100 * (y - x**2)**2

def grad_rosenbrock(w):
    """ Gradient of Rosenbrock function """
    x, y = w
    dfdx = -2 * (1 - x) - 400 * x * (y - x**2)
    dfdy = 200 * (y - x**2)
    return np.array([dfdx, dfdy])

def quadratic(w, a=10):
    """ Strongly convex quadratic function. Minimum at [0,0]. Parameter a>0. """
    x, y = w
    return 0.5 * (a * x**2 + y**2)

def grad_quadratic(w, a=10):
    x, y = w
    return np.array([a * x, y])

# 2. Adaptive line search (Armijo rule as a simple example)
def simple_line_search(f, grad_f, w, d, alpha0=1.0, c=1e-4, tau=0.5):
    """ Simple backtracking Armijo line search """
    alpha = alpha0
    f0 = f(w)
    grad0 = grad_f(w)
    while f(w + alpha * d) > f0 + c * alpha * np.dot(grad0, d):
        alpha *= tau
        if alpha < 1e-10:
            break
    return alpha

# 3. Gradient Descent (å¯é€‰è‡ªé€‚åº”æ­¥é•¿ï¼Œstep_funcä¸ºè‡ªé€‚åº”æ­¥é•¿æœç´¢)
def gradient_descent(f, grad_f, w0, max_iter=1000, tol=1e-6, alpha=1e-3, use_line_search=False, **kwargs):
    w = np.array(w0, dtype=float)
    hist = [w.copy()]
    for i in range(max_iter):
        g = grad_f(w)
        if np.linalg.norm(g) < tol:
            break
        if use_line_search:
            # Adaptive line search direction is always -gradient
            step = simple_line_search(f, grad_f, w, -g)
        else:
            step = alpha
        w -= step * g
        hist.append(w.copy())
    return np.array(hist)

# 4. Stochastic Gradient Descent
def stochastic_gradient_descent(f, grad_f, w0, max_iter=1000, step=1e-3,
                                add_noise=0.0, mult_noise=0.0, a=10):
    w = np.array(w0, dtype=float)
    hist = [w.copy()]
    for i in range(max_iter):
        g = grad_f(w) if grad_f != grad_quadratic else grad_f(w, a)
        # Add noise to gradient
        g += add_noise * np.random.randn(*g.shape)
        g *= (1.0 + mult_noise * np.random.randn(*g.shape))
        w -= step * g
        hist.append(w.copy())
    return np.array(hist)

# 5. ç»˜å›¾è¾…åŠ©å‡½æ•°
def plot_level_sets_and_path(f, hist, title, xmin=-2, xmax=2, ymin=-1, ymax=3, levels=30):
    x = np.linspace(xmin, xmax, 400)
    y = np.linspace(ymin, ymax, 400)
    X, Y = np.meshgrid(x, y)
    Z = np.array([[f([i, j]) for i, j in zip(row_x, row_y)]
                  for row_x, row_y in zip(X, Y)])
    plt.figure(figsize=(7, 5))
    plt.contour(X, Y, Z, levels=levels, cmap='viridis')
    plt.plot(hist[:, 0], hist[:, 1], 'r.-', label='Path')
    plt.scatter(hist[0,0], hist[0,1], marker='o', color='blue', label='Start')
    plt.scatter(hist[-1,0], hist[-1,1], marker='x', color='black', label='End')
    plt.title(title)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.show()

# ===================== #
# ====== å®žéªŒåŒº ====== #
# ===================== #

if __name__ == '__main__':
    # å‚æ•°è®¾ç½®
    w0 = np.array([-1.5, 2.0])    # åˆå§‹ç‚¹
    max_iter = 5000               # æœ€å¤§è¿­ä»£
    tol = 1e-8                    # æ”¶æ•›å®¹å¿
    alpha = 1e-3                  # å›ºå®šæ­¥é•¿
    a_quad = 10                   # äºŒæ¬¡å‡½æ•°å‚æ•°a

    # ---- Rosenbrock: æ¢¯åº¦ä¸‹é™ä¸Žè‡ªé€‚åº”æ­¥é•¿
    hist_gd_rosen = gradient_descent(rosenbrock, grad_rosenbrock, w0, max_iter, tol, alpha, use_line_search=False)
    hist_gd_rosen_ls = gradient_descent(rosenbrock, grad_rosenbrock, w0, max_iter, tol, alpha, use_line_search=True)
    plot_level_sets_and_path(rosenbrock, hist_gd_rosen, 'Gradient Descent on Rosenbrock')
    plot_level_sets_and_path(rosenbrock, hist_gd_rosen_ls, 'Gradient Descent (+Line Search) on Rosenbrock')

    # ---- Convex Quadratic: æ¢¯åº¦ä¸‹é™ä¸Žè‡ªé€‚åº”æ­¥é•¿
    hist_gd_quad = gradient_descent(lambda w: quadratic(w, a=a_quad),
                                    lambda w: grad_quadratic(w, a=a_quad),
                                    w0, max_iter, tol, alpha, use_line_search=False)
    plot_level_sets_and_path(lambda w: quadratic(w, a=a_quad), hist_gd_quad, 'Gradient Descent on Convex Quadratic')

    # ---- SGD: Rosenbrock, éšæœºæ‰°åŠ¨
    hist_sgd_rosen = stochastic_gradient_descent(rosenbrock, grad_rosenbrock, w0, max_iter=2000, step=1e-4,
                                                 add_noise=0.1, mult_noise=0.0)
    plot_level_sets_and_path(rosenbrock, hist_sgd_rosen, 'SGD (Additive noise) on Rosenbrock')

    # ---- SGD: Convex Quadraticï¼ŒåŠ æ€§å’Œä¹˜æ€§å™ªå£°å®žéªŒ
    hist_sgd_add = stochastic_gradient_descent(quadratic, grad_quadratic, w0, max_iter=2000, step=5e-3,
                                               add_noise=1.0, mult_noise=0.0, a=a_quad)
    hist_sgd_mult = stochastic_gradient_descent(quadratic, grad_quadratic, w0, max_iter=2000, step=5e-3,
                                                add_noise=0.0, mult_noise=0.5, a=a_quad)
    plot_level_sets_and_path(lambda w: quadratic(w, a=a_quad), hist_sgd_add, 'SGD (Add noise) Quadratic')
    plot_level_sets_and_path(lambda w: quadratic(w, a=a_quad), hist_sgd_mult, 'SGD (Mult noise) Quadratic')

    # å¯¹æ¯”ä¸åŒå®žéªŒ
    # æ”¶æ•›æ›²çº¿å¯¹æ¯”
    def plot_loss_curve(f, *paths, labels):
        plt.figure()
        for path, label in zip(paths, labels):
            loss = [f(w) for w in path]
            plt.plot(loss, label=label)
        plt.yscale('log')
        plt.xlabel('iteration')
        plt.ylabel('f(w)')
        plt.legend()
        plt.title("Loss curve")
        plt.show()

    plot_loss_curve(lambda w: quadratic(w, a=a_quad),
                    hist_gd_quad, hist_sgd_add, hist_sgd_mult,
                    labels=["GD", "SGD (add noise)", "SGD (mult noise)"])

    print("Rosenbrock GD steps:", hist_gd_rosen.shape[0])
    print("Quadratic GD steps:", hist_gd_quad.shape[0])
    print("SGD (add noise) final point:", hist_sgd_add[-1])
```